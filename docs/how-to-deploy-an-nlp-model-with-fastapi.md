# å¦‚ä½•ç”¨ FastAPI éƒ¨ç½² NLP æ¨¡å‹

> åŸæ–‡ï¼š<https://www.freecodecamp.org/news/how-to-deploy-an-nlp-model-with-fastapi/>

å¦‚æœä½ ä»äº‹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ŒçŸ¥é“å¦‚ä½•éƒ¨ç½²æ¨¡å‹æ˜¯ä½ éœ€è¦æŒæ¡çš„æœ€é‡è¦çš„æŠ€èƒ½ä¹‹ä¸€ã€‚

æ¨¡å‹éƒ¨ç½²æ˜¯å°†æ¨¡å‹é›†æˆåˆ°ç°æœ‰ç”Ÿäº§ç¯å¢ƒä¸­çš„è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹å°†æ¥æ”¶è¾“å…¥ï¼Œå¹¶ä¸ºç‰¹å®šç”¨ä¾‹çš„å†³ç­–é¢„æµ‹è¾“å‡ºã€‚

> â€œåªæœ‰å½“ä¸€ä¸ªæ¨¡å‹ä¸ä¸šåŠ¡ç³»ç»Ÿå®Œå…¨é›†æˆæ—¶ï¼Œæˆ‘ä»¬æ‰èƒ½ä»å®ƒçš„é¢„æµ‹ä¸­æå–çœŸæ­£çš„ä»·å€¼â€ã€‚â€”â€”å…‹é‡Œæ–¯æ‰˜å¼—Â·è¨ç±³ä¹Œæ‹‰

ä½ å¯ä»¥é€šè¿‡ä¸åŒçš„æ–¹å¼å°†ä½ çš„ [NLP](https://hackernoon.com/your-guide-to-natural-language-processing-nlp-dw8g360f?ref=hackernoon.com) æ¨¡å‹éƒ¨ç½²åˆ°äº§å“ä¸­ï¼Œæ¯”å¦‚ä½¿ç”¨ Flaskã€Djangoã€Bottle æˆ–å…¶ä»–æ¡†æ¶ã€‚ä½†æ˜¯åœ¨ä»Šå¤©çš„æ–‡ç« ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ ****FastAPI æ„å»ºå’Œéƒ¨ç½²æ‚¨çš„ NLP æ¨¡å‹ã€‚****

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£åˆ°:

*   å¦‚ä½•å»ºç«‹ä¸€ä¸ªå°† IMDB ç”µå½±è¯„è®ºåˆ†ç±»æˆä¸åŒæƒ…ç»ªçš„ NLP æ¨¡å‹ï¼Ÿ
*   ä»€ä¹ˆæ˜¯ FastAPIï¼Œå¦‚ä½•å®‰è£…ï¼Ÿ
*   å¦‚ä½•ç”¨ FastAPI éƒ¨ç½²æ‚¨çš„æ¨¡å‹ï¼Ÿ
*   å¦‚ä½•åœ¨ä»»ä½• Python åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨æ‚¨éƒ¨ç½²çš„ NLP æ¨¡å‹ã€‚

æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸš€

## å¦‚ä½•å»ºç«‹è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹æˆ‘ä»¬çš„ NLP æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [IMDB ç”µå½±æ•°æ®é›†](https://www.kaggle.com/c/word2vec-nlp-tutorial/data?ref=hackernoon.com)æ¥æ„å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åˆ†ç±»ç”µå½±è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ã€‚ä¸‹é¢æ˜¯ä½ åº”è¯¥éµå¾ªçš„æ­¥éª¤ã€‚

### å¯¼å…¥é‡è¦çš„åŒ…

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥ä¸€äº› Python åŒ…æ¥åŠ è½½æ•°æ®ï¼Œæ¸…ç†æ•°æ®ï¼Œåˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹(åˆ†ç±»å™¨)ï¼Œå¹¶ä¿å­˜æ¨¡å‹ä»¥è¿›è¡Œéƒ¨ç½²ã€‚

```
# import important modules
import numpy as np
import pandas as pd
# sklearn modules
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB # classifier 
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    plot_confusion_matrix,
)
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# text preprocessing modules
from string import punctuation 
# text preprocessing modules
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer 
import re #regular expression
# Download dependency
for dependency in (
    "brown",
    "names",
    "wordnet",
    "averaged_perceptron_tagger",
    "universal_tagset",
):
    nltk.download(dependency)

import warnings
warnings.filterwarnings("ignore")
# seeding
np.random.seed(123
```

ä»æ•°æ®æ–‡ä»¶å¤¹åŠ è½½æ•°æ®é›†:

```
# load data
data = pd.read_csv("../data/labeledTrainData.tsv", sep='\t')
```

ç„¶åå±•ç¤ºæ•°æ®é›†çš„ä¸€ä¸ªæ ·æœ¬:

```
# show top five rows of data
data.head()
```

![0_esD8-cAwTmwoXXFY](img/7e4c71285f01cff39c09da0eadd66f84.png)

æˆ‘ä»¬çš„æ•°æ®é›†æœ‰ 3 åˆ—:

*   ****Id**** â€”è¿™æ˜¯è¯„è®ºçš„ Id
*   ****æƒ…ç»ª**** â€”è¦ä¹ˆç§¯æ(1)ï¼Œè¦ä¹ˆæ¶ˆæ(0)
*   ****å›é¡¾**** â€”è¯„è®ºç”µå½±

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥æ•°æ®é›†çš„å½¢çŠ¶:

```
# check the shape of the data
data.shape
```

(25000, 3)

è¯¥æ•°æ®é›†æœ‰ 25ï¼Œ000 æ¡è¯„è®ºã€‚

ç°åœ¨æˆ‘ä»¬éœ€è¦æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰ä»»ä½•ç¼ºå¤±å€¼:

```
# check missing values in data
data.isnull().sum()
```

id 0
æ„Ÿæ‚Ÿ 0
å›é¡¾ 0
dtype: int64

è¾“å‡ºæ˜¾ç¤ºæˆ‘ä»¬çš„æ•°æ®é›†æ²¡æœ‰ä»»ä½•ç¼ºå¤±å€¼ã€‚

### å¦‚ä½•è¯„ä»·ç­çº§åˆ†å¸ƒ

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Pandas åŒ…ä¸­çš„ ****`value_counts()`**** æ–¹æ³•æ¥è¯„ä¼°æ•°æ®é›†çš„ç±»åˆ†å¸ƒã€‚

```
# evalute news sentiment distribution
data.sentiment.value_counts()
```

1 12500
0 12500
åç§°:æƒ…æ“ï¼Œæ•°æ®ç±»å‹:int64

åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬æœ‰ç›¸åŒæ•°é‡çš„æ­£é¢å’Œè´Ÿé¢è¯„è®ºã€‚

### å¦‚ä½•å¤„ç†æ•°æ®

åœ¨åˆ†ææ•°æ®é›†ä¹‹åï¼Œä¸‹ä¸€æ­¥æ˜¯åœ¨åˆ›å»ºæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰ï¼Œå°†æ•°æ®é›†é¢„å¤„ç†æˆæ­£ç¡®çš„æ ¼å¼ã€‚

è¿™ä¸ªæ•°æ®é›†ä¸­çš„è¯„è®ºåŒ…å«äº†å¾ˆå¤šæˆ‘ä»¬åœ¨åˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ä¸éœ€è¦çš„ä¸å¿…è¦çš„å•è¯å’Œå­—ç¬¦ã€‚

æˆ‘ä»¬å°†é€šè¿‡åˆ é™¤åœç”¨å­—è¯ã€æ•°å­—å’Œæ ‡ç‚¹æ¥æ¸…ç†é‚®ä»¶ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ NLTK åŒ…ä¸­çš„è¯æ±‡åŒ–è¿‡ç¨‹å°†æ¯ä¸ªå•è¯è½¬æ¢æˆå®ƒçš„åŸºæœ¬å½¢å¼ã€‚

****`text_cleaning()`**** å‡½æ•°å°†å¤„ç†æ‰€æœ‰å¿…è¦çš„æ­¥éª¤æ¥æ¸…ç†æˆ‘ä»¬çš„æ•°æ®é›†ã€‚

```
stop_words =  stopwords.words('english')
def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):
    # Clean the text, with the option to remove stop_words and to lemmatize word
    # Clean the text
    text = re.sub(r"[^A-Za-z0-9]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text =  re.sub(r'http\S+',' link ', text)
    text = re.sub(r'\b\d+(?:\.\d+)?\s+', '', text) # remove numbers

    # Remove punctuation from text
    text = ''.join([c for c in text if c not in punctuation])

    # Optionally, remove stop words
    if remove_stop_words:
        text = text.split()
        text = [w for w in text if not w in stop_words]
        text = " ".join(text)

    # Optionally, shorten words to their stems
    if lemmatize_words:
        text = text.split()
        lemmatizer = WordNetLemmatizer() 
        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]
        text = " ".join(lemmatized_words)

    # Return a list of words
    return(text)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ ****text_cleaning()**** å‡½æ•°æ¥æ¸…ç†æˆ‘ä»¬çš„æ•°æ®é›†:

```
#clean the review
data["cleaned_review"] = data["review"].apply(text_cleaning)
```

ç„¶åå°†æ•°æ®åˆ†æˆç‰¹å¾å˜é‡å’Œç›®æ ‡å˜é‡ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
#split features and target from  data 
X = data["cleaned_review"]
y = data.sentiment.values
```

æˆ‘ä»¬ç”¨äºè®­ç»ƒçš„ç‰¹å¾æ˜¯ ****`cleaned_review`**** å˜é‡ï¼Œç›®æ ‡æ˜¯ ****`sentiment`**** å˜é‡ã€‚

ç„¶åï¼Œæˆ‘ä»¬å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚æµ‹è¯•è§„æ¨¡æ˜¯æ•´ä¸ªæ•°æ®é›†çš„ 15%ã€‚

```
# split data into train and validate
X_train, X_valid, y_train, y_valid = train_test_split(
    X,
    y,
    test_size=0.15,
    random_state=42,
    shuffle=True,
    stratify=y,
)
```

### å¦‚ä½•åˆ›å»º NLP æ¨¡å‹

æˆ‘ä»¬å°†è®­ç»ƒå¤šé¡¹å¼[æœ´ç´ è´å¶æ–¯](https://www.freecodecamp.org/news/how-naive-bayes-classifiers-work/)ç®—æ³•æ¥åˆ†ç±»è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ã€‚è¿™æ˜¯æ–‡æœ¬åˆ†ç±»æœ€å¸¸ç”¨çš„ç®—æ³•ä¹‹ä¸€ã€‚

ä½†æ˜¯åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬æ¸…ç†è¿‡çš„è¯„è®ºè½¬æ¢æˆæ•°å€¼ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿç†è§£è¿™äº›æ•°æ®ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä» scikit-learn ä¸­ä½¿ç”¨ [****`TfidfVectorizer`**** çš„æ–¹æ³•ã€‚TfidfVectorizer å°†å¸®åŠ©æˆ‘ä»¬å°†ä¸€ç»„æ–‡æœ¬æ–‡æ¡£è½¬æ¢ä¸º TF-IDF ç‰¹å¾çŸ©é˜µã€‚](https://www.freecodecamp.org/news/how-to-extract-keywords-from-text-with-tf-idf-and-pythons-scikit-learn-b2a0f3d7e667/)

ä¸ºäº†åº”ç”¨è¿™ä¸€ç³»åˆ—æ­¥éª¤(é¢„å¤„ç†å’Œè®­ç»ƒ)ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª scikit-learn çš„ä¸€ä¸ª[ç®¡é“ç±»](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?ref=hackernoon.com)ï¼Œå®ƒé¡ºåºåº”ç”¨ä¸€ç³»åˆ—è½¬æ¢å’Œä¸€ä¸ªæœ€ç»ˆä¼°è®¡å™¨ã€‚

```
# Create a classifier in pipeline
sentiment_classifier = Pipeline(steps=[
                               ('pre_processing',TfidfVectorizer(lowercase=False)),
                                 ('naive_bayes',MultinomialNB())
                                 ])
```

ç„¶åæˆ‘ä»¬åƒè¿™æ ·è®­ç»ƒæˆ‘ä»¬çš„åˆ†ç±»å™¨:

```
# train the sentiment classifier 
sentiment_classifier.fit(X_train,y_train)
```

ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®éªŒè¯é›†åˆ›å»ºä¸€ä¸ªé¢„æµ‹:

```
# test model performance on valid data 
y_preds = sentiment_classifier.predict(X_valid)
```

å°†ä½¿ç”¨ ****`accuracy_score`**** è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨ accuracy_score æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨æƒ…æ„Ÿå˜é‡ä¸­æœ‰ç›¸åŒæ•°é‡çš„ç±»ã€‚

```
accuracy_score(y_valid,y_preds)
```

0.8629333333333333

æˆ‘ä»¬çš„æ¨¡å‹çš„å‡†ç¡®åº¦åœ¨ ****86.29%**** å·¦å³ï¼Œè¿™æ˜¯å¾ˆå¥½çš„æ€§èƒ½ã€‚

### å¦‚ä½•ä¿å­˜æ¨¡å‹ç®¡çº¿

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ****`joblib`**** Python åŒ…å°†æ¨¡å‹ç®¡é“ä¿å­˜åœ¨æ¨¡å‹çš„ç›®å½•ä¸‹ã€‚

```
#save model 
import joblib 
joblib.dump(sentiment_classifier, '../models/sentiment_model_pipeline.pkl')
```

ç°åœ¨æˆ‘ä»¬å·²ç»æ„å»ºäº† NLP æ¨¡å‹ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•ä½¿ç”¨ FastAPIã€‚

## ä»€ä¹ˆæ˜¯ FastAPIï¼Ÿ

FastAPI æ˜¯ä¸€ä¸ªå¿«é€Ÿçš„ç°ä»£ Python web æ¡†æ¶ï¼Œç”¨äºæ„å»ºä¸åŒçš„[API](https://hackernoon.com/how-to-use-the-requests-python-library-to-make-an-api-call-and-save-it-as-a-pandas-dataframe-z43k33rm?ref=hackernoon.com)ã€‚å®ƒæä¾›äº†æ›´é«˜çš„æ€§èƒ½ï¼Œæ›´å®¹æ˜“ç¼–ç ï¼Œå¹¶ä¸”æä¾›äº†è‡ªåŠ¨å’Œäº¤äº’å¼çš„æ–‡æ¡£ã€‚

![zVaxL0LohRUpfDQhznRQ9z3y5tj1-rzu32nl](img/9f16c034d92626e9680cbbf414da6c7c.png)

FastAPI å»ºç«‹åœ¨ä¸¤ä¸ªä¸»è¦çš„ Python åº“ ****â€” Starlette**** (ç”¨äº web å¤„ç†)å’Œ ****Pydantic**** (ç”¨äºæ•°æ®å¤„ç†å’ŒéªŒè¯)ã€‚ä¸ Flask ç›¸æ¯”ï¼ŒFastAPI éå¸¸å¿«ï¼Œå› ä¸ºå®ƒå°†å¼‚æ­¥å‡½æ•°å¤„ç†ç¨‹åºå¸¦åˆ°äº†è¡¨ä¸­ã€‚

å¦‚æœä½ æƒ³äº†è§£ FastAPI çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘æ¨èä½ é˜…è¯» SebastiÃ¡n RamÃ­rez çš„è¿™ç¯‡æ–‡ç« ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•ä½¿ç”¨ FastAPI çš„ä¸€äº›ç‰¹æ€§æ¥æœåŠ¡äºæˆ‘ä»¬çš„ NLP æ¨¡å‹ã€‚

### å¦‚ä½•å®‰è£… FastAPI

é¦–å…ˆï¼Œç¡®ä¿ä½ å®‰è£…äº†æœ€æ–°ç‰ˆæœ¬çš„ FastAPI(å¸¦ pip):

```
pip install fastapi
```

ä½ è¿˜éœ€è¦ä¸€ä¸ª ASGI æœåŠ¡å™¨ç”¨äºç”Ÿäº§ï¼Œæ¯”å¦‚[uvicon](http://www.uvicorn.org/?ref=hackernoon.com)ã€‚

```
pip install uvicorn
```

## å¦‚ä½•ç”¨ FastAPI éƒ¨ç½² NLP æ¨¡å‹

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ FastAPI å°†æˆ‘ä»¬è®­ç»ƒè¿‡çš„ [NLP](https://www.freecodecamp.org/news/learn-natural-language-processing-no-experience-required/) æ¨¡å‹éƒ¨ç½²ä¸º REST APIã€‚æˆ‘ä»¬å°†æŠŠ API çš„ä»£ç ä¿å­˜åœ¨ä¸€ä¸ªåä¸º ****main.py**** çš„ Python æ–‡ä»¶ä¸­ã€‚è¿™ä¸ªæ–‡ä»¶å°†è´Ÿè´£è¿è¡Œæˆ‘ä»¬çš„ FastAPI åº”ç”¨ç¨‹åºã€‚

### å¯¼å…¥åŒ…

ç¬¬ä¸€æ­¥æ˜¯å¯¼å…¥å°†å¸®åŠ©æˆ‘ä»¬æ„å»º FastAPI åº”ç”¨ç¨‹åºå’Œè¿è¡Œ NLP æ¨¡å‹çš„åŒ…ã€‚

```
# text preprocessing modules
from string import punctuation 
# text preprocessing modules
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re  # regular expression
import os
from os.path import dirname, join, realpath
import joblib
import uvicorn
from fastapi import FastAPI
```

### å¦‚ä½•åˆå§‹åŒ– FastAPI åº”ç”¨ç¨‹åºå®ä¾‹

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ¥åˆå§‹åŒ– FastAPI åº”ç”¨ç¨‹åº:

```
app = FastAPI(
    title="Sentiment Model API",
    description="A simple API that use NLP model to predict the sentiment of the movie's reviews",
    version="0.1",
)
```

å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬å·²ç»å®šåˆ¶äº† FastAPI åº”ç”¨ç¨‹åºçš„é…ç½®ï¼ŒåŒ…æ‹¬:

*   API çš„æ ‡é¢˜
*   API çš„æè¿°ã€‚
*   API çš„ç‰ˆæœ¬ã€‚

### å¦‚ä½•åŠ è½½ NLP æ¨¡å‹

ä¸ºäº†åŠ è½½ NLP æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ****`joblib.load()`**** æ–¹æ³•å¹¶å°†è·¯å¾„æ·»åŠ åˆ°æ¨¡å‹ç›®å½•ä¸­ã€‚NLP æ¨¡å‹çš„åç§°æ˜¯ ****`sentiment_model_pipeline.pkl`**** :

```
# load the sentiment model
with open(
    join(dirname(realpath(__file__)), "models/sentiment_model_pipeline.pkl"), "rb"
) as f:
    model = joblib.load(f)
```

### å¦‚ä½•å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ¸…ç†æ•°æ®

æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬ 1 éƒ¨åˆ†ä¸­çš„åŒä¸€ä¸ªå‡½æ•° ****`text_cleaning()`**** ï¼Œå®ƒé€šè¿‡åˆ é™¤åœç”¨è¯ã€æ•°å­—å’Œæ ‡ç‚¹æ¥æ¸…é™¤è¯„è®ºæ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨ [NLTK åŒ…](https://www.freecodecamp.org/news/natural-language-processing-tutorial-with-python-nltk/)ä¸­çš„è¯æ±‡åŒ–è¿‡ç¨‹å°†æ¯ä¸ªå•è¯è½¬æ¢æˆå®ƒçš„åŸºæœ¬å½¢å¼ã€‚

```
def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):
    # Clean the text, with the option to remove stop_words and to lemmatize word
    # Clean the text
    text = re.sub(r"[^A-Za-z0-9]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"http\S+", " link ", text)
    text = re.sub(r"\b\d+(?:\.\d+)?\s+", "", text)  # remove numbers
    # Remove punctuation from text
    text = "".join([c for c in text if c not in punctuation])
    # Optionally, remove stop words
    if remove_stop_words:
        # load stopwords
        stop_words = stopwords.words("english")
        text = text.split()
        text = [w for w in text if not w in stop_words]
        text = " ".join(text)
    # Optionally, shorten words to their stems
    if lemmatize_words:
        text = text.split()
        lemmatizer = WordNetLemmatizer()
        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]
        text = " ".join(lemmatized_words)
    # Return a list of words
    return text
```

### å¦‚ä½•åˆ›å»ºé¢„æµ‹ç«¯ç‚¹

ä¸‹ä¸€æ­¥æ˜¯ç”¨ GET è¯·æ±‚æ–¹æ³•æ·»åŠ æˆ‘ä»¬çš„é¢„æµ‹ç«¯ç‚¹â€œ ****/predict-review**** â€ã€‚

```
@app.get("/predict-review")
```

> API ç«¯ç‚¹æ˜¯ä¸¤ä¸ªç³»ç»Ÿäº¤äº’æ—¶é€šä¿¡é€šé“çš„å…¥å£ç‚¹ã€‚å®ƒæŒ‡çš„æ˜¯ API å’ŒæœåŠ¡å™¨ä¹‹é—´çš„é€šä¿¡æ¥è§¦ç‚¹ã€‚

ç„¶åæˆ‘ä»¬ä¸ºè¿™ä¸ªç«¯ç‚¹å®šä¹‰ä¸€ä¸ªé¢„æµ‹å‡½æ•°ã€‚è¯¥åŠŸèƒ½çš„åç§°æ˜¯ ****`predict_sentiment()`**** å¸¦è¯„å®¡å‚æ•°ã€‚

predict _ perspective()å‡½æ•°å°†æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡:

*   æ”¶åˆ°å½±è¯„ã€‚
*   ä½¿ç”¨ ****text_cleaning()**** åŠŸèƒ½æ¸…ç†ç”µå½±è¯„è®ºã€‚
*   ä½¿ç”¨æˆ‘ä»¬çš„ NLP æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚
*   å°†é¢„æµ‹ç»“æœä¿å­˜åœ¨ ****è¾“å‡º**** å˜é‡ä¸­(0 æˆ– 1)ã€‚
*   å°†é¢„æµ‹çš„æ¦‚ç‡ä¿å­˜åœ¨ ****probas**** å˜é‡ä¸­ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºä¸¤ä½å°æ•°ã€‚
*   æœ€åï¼Œè¿”å›é¢„æµ‹å’Œæ¦‚ç‡ç»“æœã€‚

```
@app.get("/predict-review")
def predict_sentiment(review: str):
    """
    A simple function that receive a review content and predict the sentiment of the content.
    :param review:
    :return: prediction, probabilities
    """
    # clean the review
    cleaned_review = text_cleaning(review)

    # perform prediction
    prediction = model.predict([cleaned_review])
    output = int(prediction[0])
    probas = model.predict_proba([cleaned_review])
    output_probability = "{:.2f}".format(float(probas[:, output]))

    # output dictionary
    sentiments = {0: "Negative", 1: "Positive"}

    # show results
    result = {"prediction": sentiments[output], "Probability": output_probability}
    return result
```

ä»¥ä¸‹æ˜¯ ****main.py**** æ–‡ä»¶ä¸­çš„æ‰€æœ‰ä»£ç å—:

```
# text preprocessing modules
from string import punctuation
# text preprocessing modules
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re  # regular expression
import os
from os.path import dirname, join, realpath
import joblib
import uvicorn
from fastapi import FastAPI 

app = FastAPI(
    title="Sentiment Model API",
    description="A simple API that use NLP model to predict the sentiment of the movie's reviews",
    version="0.1",
)

# load the sentiment model
with open(
    join(dirname(realpath(__file__)), "models/sentiment_model_pipeline.pkl"), "rb"
) as f:
    model = joblib.load(f)

# cleaning the data
def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):
    # Clean the text, with the option to remove stop_words and to lemmatize word
    # Clean the text
    text = re.sub(r"[^A-Za-z0-9]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"http\S+", " link ", text)
    text = re.sub(r"\b\d+(?:\.\d+)?\s+", "", text)  # remove numbers

    # Remove punctuation from text
    text = "".join([c for c in text if c not in punctuation])

    # Optionally, remove stop words
    if remove_stop_words:
        # load stopwords
        stop_words = stopwords.words("english")
        text = text.split()
        text = [w for w in text if not w in stop_words]
        text = " ".join(text)

    # Optionally, shorten words to their stems
    if lemmatize_words:
        text = text.split()
        lemmatizer = WordNetLemmatizer()
        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]
        text = " ".join(lemmatized_words)

    # Return a list of words
    return text

@app.get("/predict-review")
def predict_sentiment(review: str):
    """
    A simple function that receive a review content and predict the sentiment of the content.
    :param review:
    :return: prediction, probabilities
    """
    # clean the review
    cleaned_review = text_cleaning(review)

    # perform prediction
    prediction = model.predict([cleaned_review])
    output = int(prediction[0])
    probas = model.predict_proba([cleaned_review])
    output_probability = "{:.2f}".format(float(probas[:, output]))

    # output dictionary
    sentiments = {0: "Negative", 1: "Positive"}

    # show results
    result = {"prediction": sentiments[output], "Probability": output_probability}
    return result
```

### å¦‚ä½•è¿è¡Œ API

ä»¥ä¸‹å‘½ä»¤å°†å¸®åŠ©æˆ‘ä»¬è¿è¡Œæˆ‘ä»¬åˆ›å»ºçš„ FastAPI åº”ç”¨ç¨‹åºã€‚

```
uvicorn main:app --reload
```

ä¸‹é¢æ˜¯æˆ‘ä»¬ä¸º uvicorn å®šä¹‰çš„è¿è¡Œ FastAPI åº”ç”¨ç¨‹åºçš„è®¾ç½®ã€‚

*   ****main:**** æ‹¥æœ‰ FastAPI app çš„æ–‡ä»¶ main.pyã€‚
*   ****app:**** åœ¨ main.py å†…éƒ¨ç”¨ app = FastAPI()è¡Œåˆ›å»ºçš„å¯¹è±¡ã€‚
*   ****â€”é‡æ–°åŠ è½½**** :æ¯å½“æˆ‘ä»¬ä¿®æ”¹ä»£ç æ—¶ï¼Œä½¿æœåŠ¡å™¨è‡ªåŠ¨é‡å¯ã€‚

![0_EP-YBqB9hbsOL9Rt](img/66ab7ded1c360486ec0f1f2989cec2f2.png)

FastAPI æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨äº¤äº’å¼ API æ–‡æ¡£é¡µé¢ã€‚è¦è®¿é—®å®ƒï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å¯¼èˆªè‡³[****http://127 . 0 . 0 . 1:8000/docs****](http://127.0.0.1:8000/docs)ï¼Œç„¶åæ‚¨å°†çœ‹åˆ°ç”± FastAPI è‡ªåŠ¨åˆ›å»ºçš„æ–‡æ¡£é¡µé¢ã€‚

![0_KjAI4upfMAZcUGBR](img/987faa8dc15831da026e64cd737f3da4.png)

æ–‡æ¡£é¡µé¢æ˜¾ç¤ºäº†æˆ‘ä»¬çš„ API çš„åç§°ã€æè¿°åŠå…¶ç‰ˆæœ¬ã€‚å®ƒè¿˜æ˜¾ç¤ºäº† API ä¸­æ‚¨å¯ä»¥ä¸ä¹‹äº¤äº’çš„å¯ç”¨è·¯çº¿åˆ—è¡¨ã€‚

è¦è¿›è¡Œé¢„æµ‹ï¼Œé¦–å…ˆç‚¹å‡» ****é¢„æµ‹-å›é¡¾**** è·¯çº¿ç„¶åç‚¹å‡» ****æŒ‰é’®è¯•èµ°**** ã€‚è¿™å…è®¸æ‚¨å¡«å…… review å‚æ•°å¹¶ç›´æ¥ä¸ API äº¤äº’ã€‚

![0__XOHgx7DAjF74r1N](img/cb55c6c78e11e3bc4fff08da13d841f1.png)

é€šè¿‡æ·»åŠ æ‚¨é€‰æ‹©çš„ç”µå½±è¯„è®ºæ¥å¡«å†™è¯„è®ºæ ã€‚æˆ‘è¡¥å……äº†ä»¥ä¸‹å…³äº 2021 å¹´ä¸Šæ˜ çš„ ****ã€æ‰å…‹Â·æ–½å¥ˆå¾·ç‰ˆæ­£ä¹‰è”ç›Ÿã€‘**** ç”µå½±çš„å½±è¯„ã€‚

> â€œæˆ‘ä»å¤´åˆ°å°¾éƒ½å¾ˆå–œæ¬¢è¿™éƒ¨ç”µå½±ã€‚å°±åƒé›·Â·è´¹å¸Œå°”è¯´çš„ï¼Œæˆ‘å¸Œæœ›è¿™éƒ¨ç”µå½±ä¸ä¼šç»“æŸã€‚ä¹è®¨çš„åœºæ™¯ä»¤äººå…´å¥‹ï¼Œæˆ‘éå¸¸å–œæ¬¢é‚£ä¸ªåœºæ™¯ã€‚ä¸åƒã€Šæ­£ä¹‰è”ç›Ÿã€‹è¿™éƒ¨ç”µå½±å±•ç¤ºäº†æ¯ä¸ªè‹±é›„æœ€æ“…é•¿è‡ªå·±çš„äº‹æƒ…ï¼Œè®©æˆ‘ä»¬çƒ­çˆ±æ¯ä¸€ä¸ªè§’è‰²ã€‚è°¢è°¢æ‰å…‹å’Œæ•´ä¸ªå›¢é˜Ÿã€‚â€

ç„¶åç‚¹å‡»æ‰§è¡ŒæŒ‰é’®è¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°ç»“æœã€‚

![0_QAM6VjtcDlhgEbQ6](img/474c859481e578f044d8a363ec54004c.png)

æœ€åï¼Œæ¥è‡ª API çš„ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„ NLP æ¨¡å‹é¢„æµ‹æ‰€æä¾›çš„è¯„è®ºå…·æœ‰**æ­£é¢æƒ…æ„Ÿï¼Œæ¦‚ç‡ä¸º ****0.70**** :**

**![0_YUFkGlke1PWVoeyo](img/bbd4df39aba9077b542b4a4fb7f17d79.png)**

## **å¦‚ä½•åœ¨ä»»ä½• Python åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ NLP æ¨¡å‹**

**è¦åœ¨ä»»ä½• Python åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨æˆ‘ä»¬çš„ NLP APIï¼Œæˆ‘ä»¬éœ€è¦å®‰è£… requests Python åŒ…ã€‚è¿™ä¸ªåŒ…å°†å¸®åŠ©æˆ‘ä»¬å‘æˆ‘ä»¬å¼€å‘çš„ FastAPI åº”ç”¨ç¨‹åºå‘é€ HTTP è¯·æ±‚ã€‚**

**è¦å®‰è£…è¯·æ±‚åŒ…ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤:**

```
`pip install requests`
```

**ç„¶ååˆ›å»ºä¸€ä¸ªç®€å•çš„ Python æ–‡ä»¶å«åš ****`python_app.py`**** ã€‚è¿™ä¸ªæ–‡ä»¶å°†è´Ÿè´£å‘é€æˆ‘ä»¬çš„ HTTP è¯·æ±‚ã€‚**

**æˆ‘ä»¬é¦–å…ˆå¯¼å…¥è¯·æ±‚åŒ…:**

```
`import requests as r`
```

**è¡¥å……ä¸€ä¸ªå…³äº ****å“¥æ–¯æ‹‰å¤§æˆ˜å­”ã€2021ã€‘****ç”µå½±çš„å½±è¯„:**

```
`# add review
review = "This movie was exactly what I wanted in a Godzilla vs Kong movie. It's big loud, brash and dumb, in the best ways possible. It also has a heart in a the form of Jia (Kaylee Hottle) and a superbly expressionful Kong. The scenes of him in the hollow world are especially impactful and beautifully shot/animated. Kong really is the emotional core of the film (with Godzilla more of an indifferent force of nature), and is done so well he may even convert a few members of Team Godzilla."`
```

**ç„¶ååœ¨è¦ä¼ é€’ç»™ HTTP è¯·æ±‚çš„å…³é”®å‚æ•°ä¸­æ·»åŠ å®¡æŸ¥:**

```
`keys = {"review": review}`
```

**æœ€åï¼Œæˆ‘ä»¬å‘æˆ‘ä»¬çš„ API å‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œå¯¹è¯„è®ºè¿›è¡Œé¢„æµ‹:**

```
`prediction = r.get("http://127.0.0.1:8000/predict-review/", params=keys)`
```

**ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é¢„æµ‹ç»“æœ:**

```
`results = prediction.json()

print(results["prediction"])
print(results["Probability"])`
```

**è¿™å°†æ˜¾ç¤ºé¢„æµ‹åŠå…¶æ¦‚ç‡ã€‚ç»“æœå¦‚ä¸‹:**

**æ­£
0.54**

## **åŒ…æ‰**

**æ­å–œğŸ‘ğŸ‘ï¼Œä½ å·²ç»ç†¬åˆ°è¿™ç¯‡æ–‡ç« çš„ç»“å°¾äº†ã€‚æˆ‘å¸Œæœ›æ‚¨å·²ç»å­¦åˆ°äº†ä¸€äº›æ–°ä¸œè¥¿ï¼Œç°åœ¨çŸ¥é“å¦‚ä½•ç”¨ FastAPI éƒ¨ç½²æ‚¨çš„ NLP æ¨¡å‹ã€‚**

**å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äº FastAPI çš„çŸ¥è¯†ï¼Œæˆ‘æ¨èä½ å‚åŠ ç”±[bitfuses](https://twitter.com/bitfumes?ref=hackernoon.com)åˆ›å»ºçš„[å®Œæ•´ FastAPI è¯¾ç¨‹](https://www.youtube.com/watch?v=7t2alSnE2-I&ref=hackernoon.com)ã€‚**

**ä½ å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½æœ¬æ–‡ä½¿ç”¨çš„[é¡¹ç›®æºä»£ç ã€‚](https://github.com/Davisy/Deploy-NLP-Model-with-FastAPI)**

**å¦‚æœä½ å­¦åˆ°äº†æ–°çš„ä¸œè¥¿æˆ–è€…å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œè¯·åˆ†äº«ç»™å…¶ä»–äººçœ‹ã€‚åœ¨é‚£ä¹‹å‰ï¼Œä¸‹ä¸€ç¯‡æ–‡ç« å†è§ï¼ã€‚**

**ä½ ä¹Ÿå¯ä»¥åœ¨æ¨ç‰¹ä¸Šæ‰¾åˆ°æˆ‘ [@Davis_McDavid](https://twitter.com/Davis_McDavid?ref=hackernoon.com)**