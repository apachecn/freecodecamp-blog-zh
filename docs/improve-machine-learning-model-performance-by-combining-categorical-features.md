# å¦‚ä½•é€šè¿‡ç»„åˆåˆ†ç±»ç‰¹å¾æ¥æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½

> åŸæ–‡ï¼š<https://www.freecodecamp.org/news/improve-machine-learning-model-performance-by-combining-categorical-features/>

å½“æ‚¨è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæ‚¨çš„æ•°æ®é›†ä¸­å¯ä»¥æœ‰ä¸€äº›è¡¨ç¤ºåˆ†ç±»å€¼çš„è¦ç´ ã€‚åˆ†ç±»è¦ç´ æ˜¯å¯ä»¥åˆ†ç»„çš„æ•°æ®ç±»å‹ã€‚

æœ‰ä¸‰ç§å¸¸è§çš„åˆ†ç±»æ•°æ®ç±»å‹:

1.  **åºæ•°**â€“ä»¥å‡åºæˆ–é™åºæ’åˆ—çš„ä¸€ç»„å€¼ã€‚ä¾‹å­:ç”¨ 1-10 çš„å°ºåº¦ç»™å¹¸ç¦æ‰“åˆ†
2.  **äºŒè¿›åˆ¶**â€“åªæœ‰ä¸¤ä¸ªå€¼çš„é›†åˆã€‚ä¾‹å¦‚:çƒ­æˆ–å†·ã€‚
3.  **åä¹‰**â€“åŒ…å«æ²¡æœ‰ç‰¹å®šé¡ºåºçš„å€¼çš„é›†åˆã€‚ç¤ºä¾‹:å›½å®¶åˆ—è¡¨

å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•éœ€è¦æ•°å­—è¾“å…¥å’Œè¾“å‡ºå˜é‡ã€‚è¿™æ„å‘³ç€æ‚¨å¿…é¡»å°†æ•°æ®é›†ä¸­çš„åˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼Œä»¥ä¾¿æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥ä½¿ç”¨å®ƒä»¬ã€‚

æ‚¨å¯ä»¥å¯¹äºŒè¿›åˆ¶ç‰¹å¾ä½¿ç”¨[æ ‡ç­¾ç¼–ç ](https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/?ref=hackernoon.com)ï¼Œæˆ–è€…å¯¹åä¹‰ç‰¹å¾ä½¿ç”¨[ä¸€æ¬¡çƒ­ç¼–ç ](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f?ref=hackernoon.com)æ–¹æ³•ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£ç»„åˆåˆ†ç±»ç‰¹å¾å¦‚ä½•æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚

æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸš€

## å¦‚ä½•ç»„åˆæœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„åˆ†ç±»ç‰¹å¾

æ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªç”±å…¶ä»–ä¸¤ä¸ªåˆ†ç±»è¦ç´ ç»„åˆè€Œæˆçš„æ–°è¦ç´ ã€‚æ‚¨è¿˜å¯ä»¥ç»„åˆä¸‰ä¸ªæˆ–å››ä¸ªä»¥ä¸Šç”šè‡³æ›´å¤šçš„åˆ†ç±»ç‰¹å¾ã€‚

```
df["new_feature"] = (
	df.feature_1.astype(str)
	 + "_"
	 + df.feature_2.astype(str)
	)
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ Pandas åˆå¹¶ä¸¤ä¸ªåˆ†ç±»è¦ç´ ï¼Œå¹¶åœ¨æ•°æ®é›†ä¸­å½¢æˆä¸€ä¸ªæ–°è¦ç´ ã€‚

é‚£ä¹ˆä½ åº”è¯¥ç»“åˆå“ªäº›åˆ†ç±»ç‰¹å¾å‘¢ï¼Ÿè¿™ä¸ªé—®é¢˜æ²¡æœ‰ç®€å•çš„ç­”æ¡ˆã€‚è¿™å–å†³äºæ‚¨çš„æ•°æ®å’Œè¦ç´ ç±»å‹ã€‚ä¸€äº›é¢†åŸŸçŸ¥è¯†å¯èƒ½å¯¹åˆ›å»ºè¿™æ ·çš„æ–°ç‰¹æ€§æœ‰ç”¨ã€‚

ä¸ºäº†è¯´æ˜æ•´ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª [Zindi ç«èµ›é¡µé¢](https://zindi.africa/competitions/financial-inclusion-in-africa?ref=hackernoon.com)çš„[éæ´²é‡‘èåŒ…å®¹æ€§æ•°æ®é›†](https://zindi.africa/competitions/financial-inclusion-in-africa/data?ref=hackernoon.com)ã€‚å®ƒæœ‰è®¸å¤šåˆ†ç±»ç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ç»“åˆèµ·æ¥ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥æ”¹è¿›æ¨¡å‹çš„æ€§èƒ½ã€‚

è¯¥æ•°æ®é›†çš„ç›®æ ‡æ˜¯é¢„æµ‹è°æœ€æœ‰å¯èƒ½æ‹¥æœ‰é“¶è¡Œå¸æˆ·ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ã€‚

## æ­¥éª¤ 1â€“åŠ è½½æ•°æ®é›†

æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯ç¡®ä¿æˆ‘ä»¬å·²ç»ä¸‹è½½äº†æ¯”èµ›ä¸­æä¾›çš„æ•°æ®é›†ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½æ•°æ®é›†[ã€‚](https://zindi.africa/competitions/financial-inclusion-in-africa/data?ref=hackernoon.com)

åƒè¿™æ ·å¯¼å…¥é‡è¦çš„ Python åŒ…:

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
np.random.seed(123)
warnings.filterwarnings('ignore')
%matplotlib inline 
```

ç„¶ååŠ è½½æ•°æ®é›†ã€‚

```
# Import data
data = pd.read_csv('data/Train_v2.csv') 
```

è®©æˆ‘ä»¬çœ‹çœ‹æ•°æ®é›†çš„å½¢çŠ¶:

```
# print shape
print('data shape :', data.shape)

data shape : (23524, 13) 
```

ä¸Šé¢çš„è¾“å‡ºæ˜¾ç¤ºäº†æ•°æ®é›†ä¸­çš„è¡Œæ•°å’Œåˆ—æ•°ã€‚æ•°æ®é›†ä¸­æœ‰ 13 ä¸ªå˜é‡â€”â€”12 ä¸ªè‡ªå˜é‡ï¼Œ1 ä¸ªå› å˜é‡ã€‚

é€šè¿‡ä½¿ç”¨ Pandas åº“ä¸­çš„ **`head()`** æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°æ®é›†ä¸­çš„å‰äº”è¡Œã€‚

```
# inspect data 

data.head()
```

![zVaxL0LohRUpfDQhznRQ9z3y5tj1-oz2132qe](img/7d0234c5ba39eeabc10308daf7c3d31d.png)

ç†è§£æ¯ä¸ªç‰¹å¾çš„å«ä¹‰éå¸¸é‡è¦ï¼Œè¿™æ ·æ‚¨æ‰èƒ½çœŸæ­£ç†è§£æ•°æ®é›†ã€‚æ‚¨å¯ä»¥é˜…è¯» **VariableDefinition.csv** æ–‡ä»¶æ¥ç†è§£æ•°æ®é›†ä¸­å‡ºç°çš„æ¯ä¸ªå˜é‡çš„å«ä¹‰ã€‚

## æ­¥éª¤ 2â€“è§£é‡Šæ•°æ®é›†

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ **`info()`** æ–¹æ³•ä»ç†ŠçŒ«èº«ä¸Šè·å¾—æ›´å¤šå…³äºæˆ‘ä»¬æ‰€æ‹¥æœ‰çš„ç‰¹å¾çš„ä¿¡æ¯ã€‚

```
#show Some information about the dataset

print(train_data.info())
```

![zVaxL0LohRUpfDQhznRQ9z3y5tj1-h62f32t7](img/154071b327f4b0dd1f7f2822b4f491dd.png)

è¾“å‡ºæ˜¾ç¤ºå˜é‡/ç‰¹å¾åˆ—è¡¨ã€å¤§å°(å¦‚æœåŒ…å«ç¼ºå¤±å€¼)ä»¥åŠæ¯ä¸ªå˜é‡çš„æ•°æ®ç±»å‹ã€‚

æ•°æ®é›†ä¸­æ²¡æœ‰ä»»ä½•å€¼ä¸¢å¤±ã€‚æˆ‘ä»¬æœ‰ 3 ä¸ªæ•´æ•°æ•°æ®ç±»å‹çš„ç‰¹æ€§å’Œ 10 ä¸ªå¯¹è±¡æ•°æ®ç±»å‹çš„ç‰¹æ€§(å¤§å¤šæ•°æ˜¯åˆ†ç±»ç‰¹æ€§)ã€‚

## æ­¥éª¤ 3â€“ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å‡†å¤‡æ•°æ®

ä¸‹ä¸€æ­¥æ˜¯ä»æ•°æ®ä¸­åˆ†ç¦»å‡ºè‡ªå˜é‡å’Œç›®æ ‡(bank_account)ã€‚ç„¶åä½¿ç”¨ [LabelEncoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd?ref=hackernoon.com) å°†ç›®æ ‡å€¼ä»å¯¹è±¡æ•°æ®ç±»å‹è½¬æ¢æˆæ•°å­—ã€‚

```
#import preprocessing module
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

# Convert target label to numerical Data
le = LabelEncoder()
data['bank_account'] = le.fit_transform(data['bank_account'])

#Separate training features from target
X = data.drop(['bank_account'], axis=1)
y = data['bank_account']

print(y) 
```

![zVaxL0LohRUpfDQhznRQ9z3y5tj1-el2r32yy--1-](img/bd764781da40a67799f56fecce219c00.png)

æˆ‘ä»¬å·²ç»å°†ç›®æ ‡å€¼è½¬æ¢ä¸ºæ•°å­—æ•°æ®ç±»å‹â€”â€”1 ä»£è¡¨â€œæ˜¯â€, 0 ä»£è¡¨â€œå¦â€ã€‚

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„é¢„å¤„ç†å‡½æ•°æ¥:

*   å¤„ç†æ•°æ®ç±»å‹çš„è½¬æ¢ã€‚
*   ä½¿ç”¨[ä¸€é”®ç¼–ç å™¨å’Œ/æˆ–æ ‡ç­¾ç¼–ç å™¨](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd?ref=hackernoon.com)å°†åˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºæ•°å­—ç‰¹å¾ã€‚
*   åˆ é™¤ uniqueid å˜é‡ã€‚
*   æ‰§è¡Œ[ç‰¹å¾ç¼©æ”¾](https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9?ref=hackernoon.com)ã€‚

```
# function to preprocess our data 

def preprocessing_data(data):

    # Convert the following numerical labels from interger to float
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(float
    )

    # categorical features to be converted to One Hot Encoding
    categ = [
        "relationship_with_head",
        "marital_status",
        "education_level",
        "job_type",
        "country",
    ]

    # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)

    # Label Encoder conversion
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])

    # drop uniquid column
    data = data.drop(["uniquid"]), axis=1)

    # scale our data 
    scaler = StandardScaler()
    data = scaler.fit_transform(data)

    return data 
```

è®©æˆ‘ä»¬é¢„å¤„ç†æˆ‘ä»¬çš„æ•°æ®é›†ã€‚

```
# preprocess the train data 
processed_test_data = preprocessing_data(X_train) 
```

## æ­¥éª¤ 4â€“æ¨¡å‹æ„å»ºå’Œå®éªŒ

æˆ‘ä»¬å°†ä½¿ç”¨æ•°æ®é›†çš„ä¸€éƒ¨åˆ†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

```
# Split train_data
from sklearn.model_selection import train_test_spilt
X_Train, X_val, y_Train, y_val = train_test_split(processed_train_data, y_train, stratify = y, test_size = 0.1, random_state=42) 
```

æˆ‘ä»¬å°†åªä½¿ç”¨æ•°æ®é›†çš„ **10%** æ¥è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å‚æ•°**åˆ†å±‚= y** å°†ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ä¸¤ä¸ªç±»çš„å€¼ç›¸ç­‰(â€œæ˜¯â€å’Œâ€œå¦â€)ã€‚

å¯¹äºè¿™ä¸ªåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**é€»è¾‘å›å½’ç®—æ³•**æ¥è®­ç»ƒå’Œé¢„æµ‹è°æœ€æœ‰å¯èƒ½æ‹¥æœ‰é“¶è¡Œè´¦æˆ·ã€‚

```
#import classifier algorithm here
from sklearn.linear_model import LogisticRegression

# create classifier
lg_model = LogisticRegression()

#Training the classifier
lg_model.fit(X_Train,y_Train) 
```

è®­ç»ƒå®Œåˆ†ç±»å™¨ä¹‹åï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¥é¢„æµ‹æˆ‘ä»¬çš„è¯„ä¼°é›†ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®æ€§ä½œä¸ºæˆ‘ä»¬çš„è¯„ä¼°æ ‡å‡†ã€‚

```
# import evaluation metrics
from sklearn.metrics import confusion_matrix, accuracy_score

# evaluate the model
y_pred = lg_model.predict(X_val)

# Get the accuracy
print("Accuracy Score of Logistic Regression classifier: ","{:.4f}".format(accuracy_score(y_val, lg_y_pred))) 
```

æˆ‘ä»¬ä»é€»è¾‘å›å½’åˆ†ç±»å™¨å¾—åˆ°çš„å‡†ç¡®åº¦åˆ†æ•°ä¸º **0.8874** ã€‚

## å¦‚ä½•ç»“åˆ`education_level`å’Œ`job_type`ç‰¹æ€§æ¥æé«˜æ€§èƒ½

ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†åŸºæœ¬æ¨¡å‹çš„æ€§èƒ½ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡ç»“åˆ **`education_level`** å’Œ **`job_type`** ç‰¹æ€§æ¥æ”¹è¿›å®ƒã€‚

åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ–°æˆ‘ä»¬å·²ç»åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°ï¼Œç„¶åè¿è¡Œå‰©ä½™çš„ä»£ç ã€‚

```
# function to preprocess our data 

def preprocessing_data(data):

    # Convert the following numerical labels from integer to float
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(float)

    # combine some cat features 
    data["features_combination"] = (data.education_level.astype(str) + "_" + data.job_type.astype(str) )

    # remove individual features that are combined together
    data = data.drop(['education_level','job_type'], axis=1)

    # categorical features to be converted by One Hot Encoding
    categ = [
      "relationship_with_head",
      "marital_status",
      "features_combination",
      "country"
      ]

    # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)

    # Label Encoder conversion
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])

    # drop uniquid column
    data = data.drop(["uniqueid"], axis=1)

    # scale our data 
    scaler = StandardScaler()
    data = scaler.fit_transform(data)

    return data 
```

åœ¨ä¸Šé¢çš„é¢„å¤„ç†å‡½æ•°ä¸­ï¼Œæˆ‘é€šè¿‡ä»¥ä¸‹æ–¹å¼æ›´æ–°äº†ä»£ç :

*   ç»“åˆ`education_level`å’Œ`job_type`åˆ›å»ºä¸€ä¸ªåä¸º **`features_combination`** çš„æ–°ç‰¹å¾ã€‚
*   ä»æ•°æ®é›†ä¸­ç§»é™¤å•ä¸ªè¦ç´ (` education_level 'å’Œ`job_type`)ã€‚
*   åœ¨**ä¸€ä¸ªçƒ­ç¼–ç **å°†è½¬æ¢çš„åˆ†ç±»ç‰¹å¾åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªåä¸º **`feature_combinaton`** çš„æ–°ç‰¹å¾ã€‚

**æ³¨æ„:**æˆ‘åªé€‰æ‹©äº†åä¹‰åˆ†ç±»ç‰¹å¾(å…·æœ‰ä¸¤ä¸ªä»¥ä¸Šçš„å”¯ä¸€å€¼)ã€‚

åœ¨ä¸ºè¯¥å®éªŒé‡æ–°è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨ä¹‹åï¼Œæ¨¡å‹æ€§èƒ½ä» **0.8874** æé«˜åˆ° **0.8882** ã€‚è¿™è¡¨æ˜ç»“åˆåˆ†ç±»ç‰¹å¾å¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

è¯·è®°ä½ï¼Œæˆ‘ä»¬æ²¡æœ‰æ”¹å˜ä»»ä½•ä¸œè¥¿ï¼Œå¦‚æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ä¸­çš„è¶…å‚æ•°ã€‚

## å¦‚ä½•ç»“åˆ`relation_with_head`å’Œ`marital_status`ç‰¹æ€§æ¥æé«˜æ€§èƒ½

åœ¨æˆ‘ä»¬çš„ç¬¬äºŒä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬å°†ç»“åˆå¦å¤–ä¸¤ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œå®ƒä»¬æ˜¯ **`relationship_with_head`** å’Œ **`marital_status`** ã€‚

æˆ‘ä»¬åªéœ€è¦æ›´æ–°é¢„å¤„ç†å‡½æ•°(å°±åƒç¬¬ä¸€ä¸ªå®éªŒä¸€æ ·)ï¼Œç„¶åè¿è¡Œå‰©ä¸‹çš„ä»£ç ã€‚

```
# function to preprocess our data 

def preprocessing_data(data):

    # Convert the following numerical labels from integer to float
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(
        float
    )

    # combine some cat features 
    data["features_combination"] = (data.relationship_with_head.astype(str) + "_"
                           + data.marital_status.astype(str) 
                      )
    # remove individual features that are combined together
    data = data.drop(['relationship_with_head','marital_status'], axis=1)

    # categorical features to be converted by One Hot Encoding
    categ = [
        "features_combination",
        "education_level",
        "job_type",
        "country",
    ]

    # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)

    # Label Encoder conversion
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])

    # drop uniquid column
    data = data.drop(["uniqueid"], axis=1)

    # scale our data 
    scaler = StandardScaler()
    data = scaler.fit_transform(data)

    return data
```

åœ¨ä¸Šè¿°é¢„å¤„ç†å‡½æ•°ä¸­ï¼Œæˆ‘é€šè¿‡ä»¥ä¸‹æ–¹å¼æ›´æ–°äº†ä»£ç 

*   ç»“åˆ`relation_with_head`å’Œ`marital_status`åˆ›å»ºä¸€ä¸ªåä¸º **`features_combination`** çš„æ–°ç‰¹å¾ã€‚
*   ä»æ•°æ®é›†ä¸­ç§»é™¤å•ä¸ªç‰¹å¾(` relation_with_head 'å’Œ`marital_status`)ã€‚
*   åœ¨**ä¸€ä¸ªçƒ­ç¼–ç **å°†è½¬æ¢çš„åˆ†ç±»ç‰¹å¾åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªåä¸º **`feature_combination`** çš„æ–°ç‰¹å¾ã€‚

åœ¨ä¸ºç¬¬äºŒä¸ªå®éªŒé‡æ–°è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨åï¼Œæ¨¡å‹æ€§èƒ½ä» **0.8874** ä¸‹é™åˆ° **0.8865** ã€‚

è¿™è¯´æ˜æœ‰æ—¶å€™å½“ä½ ç»“åˆåˆ†ç±»ç‰¹å¾çš„æ—¶å€™ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹å¹¶ä¸ä¼šåƒä½ é¢„æœŸçš„é‚£æ ·æé«˜ã€‚å› æ­¤ï¼Œä½ å°†éœ€è¦è¿è¡Œå¤§é‡çš„å®éªŒï¼Œç›´åˆ°ä½ ä»ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚

## åŒ…æ‰

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨äº†è§£äº†å¦‚ä½•ç»„åˆæ•°æ®é›†ä¸­çš„åˆ†ç±»ç‰¹å¾ï¼Œä»¥æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚

è¯·è®°ä½â€”â€”ä¸ºäº†ä»æ‚¨çš„æ¨¡å‹ä¸­è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œæ‚¨éœ€è¦å¯¹æ‚¨æ­£åœ¨è§£å†³çš„é—®é¢˜æœ‰ä¸€å®šçš„é¢†åŸŸçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæ‚¨éœ€è¦è¿è¡Œå¤§é‡éœ€è¦æ›´å¤šè®¡ç®—èµ„æºçš„å®éªŒã€‚

æ­å–œğŸ‘ğŸ‘ï¼Œä½ å·²ç»åšåˆ°è¿™ç¯‡æ–‡ç« çš„ç»“å°¾äº†ï¼æˆ‘å¸Œæœ›ä½ å­¦åˆ°äº†ä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œå¯¹ä½ çš„ä¸‹ä¸€ä¸ªæœºå™¨å­¦ä¹ æˆ–æ•°æ®ç§‘å­¦é¡¹ç›®æœ‰æ‰€å¸®åŠ©ã€‚

å¦‚æœä½ å­¦åˆ°äº†æ–°çš„ä¸œè¥¿æˆ–è€…å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œè¯·åˆ†äº«ç»™å…¶ä»–äººçœ‹ã€‚åœ¨é‚£ä¹‹å‰ï¼Œä¸‹æœŸå¸–å­å†è§ï¼

ä½ ä¹Ÿå¯ä»¥åœ¨ Twitter ä¸Šæ‰¾åˆ°æˆ‘ [@Davis_McDavid](https://twitter.com/Davis_McDavid?ref=hackernoon.com) ã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œ *é˜…è¯»[å…¶ä»–æ–‡ç« ã€‚](https://hackernoon.com/u/davisdavid)*