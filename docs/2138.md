# æœºå™¨å­¦ä¹ æ•™ç¨‹â€“é¢å‘åˆå­¦è€…çš„ç‰¹å¾å·¥ç¨‹å’Œç‰¹å¾é€‰æ‹©

> åŸæ–‡ï¼š<https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/>

ä»–ä»¬è¯´**æ•°æ®**æ˜¯æ–°çš„**çŸ³æ²¹**ï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸ç›´æ¥ä½¿ç”¨çŸ³æ²¹çš„æ¥æºã€‚åœ¨æˆ‘ä»¬å°†å®ƒç”¨äºä¸åŒç›®çš„ä¹‹å‰ï¼Œå®ƒå¿…é¡»ç»è¿‡å¤„ç†å’Œæ¸…æ´—ã€‚

è¿™åŒæ ·é€‚ç”¨äºæ•°æ®ï¼Œæˆ‘ä»¬ä¸ç›´æ¥ä»å®ƒçš„æ¥æºä½¿ç”¨å®ƒã€‚å®ƒä¹Ÿå¿…é¡»è¢«å¤„ç†ã€‚

![imageonline-co-merged-image](img/1ad88aefbf2289869bdc7ccdceeb88d7.png)

Oil industry

å¯¹äºæœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦çš„åˆå­¦è€…æ¥è¯´ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ•°æ®æ¥è‡ªä¸åŒçš„æ¥æºï¼Œå…·æœ‰ä¸åŒçš„æ•°æ®ç±»å‹ã€‚å› æ­¤ï¼Œæ‚¨ä¸èƒ½å¯¹ä¸åŒç±»å‹çš„æ•°æ®åº”ç”¨ç›¸åŒçš„æ¸…ç†å’Œå¤„ç†æ–¹æ³•ã€‚

> "å¯ä»¥ä»æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œå°±åƒå¯ä»¥ä»çŸ³æ²¹ä¸­æå–èƒ½é‡ä¸€æ ·."- [é˜¿å¾·å¥¥æ‹‰Â·é˜¿å¾·è¥¿çº³](https://medium.com/@adeolaadesina)

ä½ å¿…é¡»æ ¹æ®ä½ æ‰€æŒæ¡çš„æ•°æ®æ¥å­¦ä¹ å’Œè¿ç”¨æ–¹æ³•ã€‚ç„¶åä½ å¯ä»¥ä»ä¸­è·å¾—æ´å¯ŸåŠ›ï¼Œæˆ–è€…ç”¨å®ƒæ¥è¿›è¡Œæœºå™¨å­¦ä¹ æˆ–è€…æ·±åº¦å­¦ä¹ ç®—æ³•çš„è®­ç»ƒã€‚

çœ‹å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ ä¼šçŸ¥é“:

*   ä»€ä¹ˆæ˜¯ç‰¹å¾å·¥ç¨‹å’Œç‰¹å¾é€‰æ‹©ã€‚
*   å¤„ç†æ•°æ®é›†ä¸­ç¼ºå¤±æ•°æ®çš„ä¸åŒæ–¹æ³•ã€‚
*   å¤„ç†è¿ç»­ç‰¹å¾çš„ä¸åŒæ–¹æ³•ã€‚
*   å¤„ç†åˆ†ç±»ç‰¹å¾çš„ä¸åŒæ–¹æ³•ã€‚
*   ä¸åŒçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸš€

# ä»€ä¹ˆæ˜¯ç‰¹å¾å·¥ç¨‹ï¼Ÿ

ç‰¹å¾å·¥ç¨‹æ˜¯æŒ‡åœ¨ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ›å»º**é¢„æµ‹æ¨¡å‹**æ—¶ï¼Œé€‰æ‹©å¹¶**è½¬æ¢æ•°æ®é›†ä¸­çš„**å˜é‡/ç‰¹å¾çš„è¿‡ç¨‹ã€‚

å› æ­¤ï¼Œåœ¨ç”¨æœºå™¨å­¦ä¹ ç®—æ³•è®­ç»ƒä½ çš„æ•°æ®ä¹‹å‰ï¼Œä½ å¿…é¡»ä»ä½ å·²ç»æ”¶é›†çš„**åŸå§‹æ•°æ®é›†**ä¸­æå–ç‰¹å¾ã€‚
å¦åˆ™ï¼Œå¾ˆéš¾ä»ä½ çš„æ•°æ®ä¸­è·å¾—å¥½çš„è§è§£ã€‚

> æ‹·é—®æ•°æ®ï¼Œå®ƒä»€ä¹ˆéƒ½ä¼šæ‰¿è®¤ã€‚ç½—çº³å¾·Â·ç§‘æ–¯

ç‰¹å¾å·¥ç¨‹æœ‰ä¸¤ä¸ªç›®æ ‡:

*   å‡†å¤‡é€‚å½“çš„è¾“å…¥æ•°æ®é›†ï¼Œä¸æœºå™¨å­¦ä¹ ç®—æ³•è¦æ±‚å…¼å®¹ã€‚
*   æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„**æ€§èƒ½**ã€‚

![Picture1](img/c2cf21a4e2aa389b8cc4136bd0a19553.png)

CrowdFlower Survey

æ ¹æ® CrowdFlower å¯¹ 80 åæ•°æ®ç§‘å­¦å®¶çš„è°ƒæŸ¥ï¼Œæ•°æ®ç§‘å­¦å®¶èŠ±è´¹ **60%** çš„æ—¶é—´æ¸…ç†å’Œç»„ç»‡æ•°æ®ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‹¥æœ‰ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©çš„æŠ€èƒ½æ˜¯éå¸¸é‡è¦çš„ã€‚

> â€œæœ€ç»ˆï¼Œä¸€äº›æœºå™¨å­¦ä¹ é¡¹ç›®æˆåŠŸäº†ï¼Œä¸€äº›å¤±è´¥äº†ã€‚æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿè½»æ¾æœ€é‡è¦çš„å› ç´ å°±æ˜¯ ***çš„ç‰¹æ€§*** *çš„ä½¿ç”¨â€â€”åç››é¡¿å¤§å­¦çš„ Pedro Domingos æ•™æˆ*

å¯ä»¥ä»ä»¥ä¸‹é“¾æ¥é˜…è¯»ä»–çš„è®ºæ–‡:[å…³äºæœºå™¨å­¦ä¹ è¦çŸ¥é“çš„å‡ ä»¶æœ‰ç”¨çš„äº‹æƒ…](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)ã€‚

æ—¢ç„¶æ‚¨çŸ¥é“äº†ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ä¸åŒçš„ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬ä»å­¦ä¹ å¤„ç†ç¼ºå¤±æ•°æ®çš„ä¸åŒæ–¹æ³•å¼€å§‹ã€‚

## å¦‚ä½•å¤„ç†ä¸¢å¤±çš„æ•°æ®

å¤„ç†ç¼ºå¤±æ•°æ®éå¸¸é‡è¦ï¼Œå› ä¸ºè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•ä¸æ”¯æŒå…·æœ‰ç¼ºå¤±å€¼çš„æ•°æ®ã€‚å¦‚æœæ•°æ®é›†ä¸­æœ‰ç¼ºå¤±å€¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸€äº›æœºå™¨å­¦ä¹ ç®—æ³•å‡ºé”™å’Œæ€§èƒ½ä¸‹é™ã€‚

ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸­æ‰¾åˆ°çš„å¸¸è§ç¼ºå¤±å€¼åˆ—è¡¨ã€‚

*   ä¸é€‚ç”¨çš„
*   ç©º
*   ç©ºçš„
*   ï¼Ÿ
*   æ²¡æœ‰äºº
*   ç©ºçš„
*   -
*   åœ†ç›˜çƒ¤é¥¼

è®©æˆ‘ä»¬å­¦ä¹ ä¸åŒçš„æ–¹æ³•æ¥è§£å†³ç¼ºå¤±æ•°æ®çš„é—®é¢˜ã€‚

### å˜é‡åˆ é™¤

å˜é‡åˆ é™¤åŒ…æ‹¬æ ¹æ®å…·ä½“æƒ…å†µåˆ é™¤ç¼ºå°‘å€¼çš„å˜é‡(åˆ—)ã€‚å½“å˜é‡ä¸­æœ‰å¾ˆå¤šç¼ºå¤±å€¼ï¼Œå¹¶ä¸”å˜é‡ç›¸å¯¹ä¸å¤ªé‡è¦æ—¶ï¼Œè¿™ç§æ–¹æ³•æ˜¯æœ‰æ„ä¹‰çš„ã€‚

å”¯ä¸€å€¼å¾—åˆ é™¤å˜é‡çš„æƒ…å†µæ˜¯å½“å®ƒçš„ç¼ºå¤±å€¼è¶…è¿‡è§‚å¯Ÿå€¼çš„ 60%æ—¶ã€‚

```
# import packages
import numpy as np 
import pandas as pd 

# read dataset 
data = pd.read_csv('path/to/data')

#set threshold
threshold = 0.7

# dropping columns with missing value rate higher than threshold
data = data[data.columns[data.isnull().mean() < threshold]] 
```

åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘å¦‚ä½•ä½¿ç”¨ NumPy å’Œ pandas æ¥åŠ è½½æ•°æ®é›†ï¼Œå¹¶å°†é˜ˆå€¼è®¾ç½®ä¸º **0.7** ã€‚è¿™æ„å‘³ç€ä»»ä½•ç¼ºå¤±å€¼è¶…è¿‡è§‚å¯Ÿå€¼çš„ **70%** çš„åˆ—éƒ½å°†ä»æ•°æ®é›†ä¸­åˆ é™¤ã€‚

æˆ‘å»ºè®®æ‚¨æ ¹æ®æ•°æ®é›†çš„å¤§å°æ¥è®¾ç½®é˜ˆå€¼ã€‚

### å‡å€¼æˆ–ä¸­ä½æ•°æ’è¡¥

å¦ä¸€ç§å¸¸è§çš„æŠ€æœ¯æ˜¯ä½¿ç”¨éç¼ºå¤±è§‚æµ‹å€¼çš„å¹³å‡å€¼æˆ–ä¸­å€¼ã€‚æ­¤ç­–ç•¥å¯åº”ç”¨äºå…·æœ‰æ•°å€¼æ•°æ®çš„è¦ç´ ã€‚

```
# filling missing values with medians of the columns
data = data.fillna(data.median())
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**ä¸­å€¼æ–¹æ³•**æ¥å¡«å……æ•°æ®é›†ä¸­ç¼ºå¤±çš„å€¼ã€‚

### æœ€å¸¸è§çš„å€¼

è¯¥æ–¹æ³•ç”¨åˆ—/ç‰¹å¾ä¸­çš„**æœ€å¤§å‡ºç°å€¼**æ›¿æ¢ç¼ºå¤±å€¼ã€‚è¿™æ˜¯å¤„ç†**åˆ†ç±»**åˆ—/ç‰¹å¾çš„ä¸€ä¸ªå¥½é€‰æ‹©ã€‚

```
# filling missing values with medians of the columns
data['column_name'].fillna(data['column_name'].value_counts().idxmax(). inplace=True)
```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ pandas çš„ **value_counts()** æ–¹æ³•æ¥è®¡ç®—åˆ—ä¸­æ¯ä¸ªå”¯ä¸€å€¼çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åç”¨æœ€å¸¸è§çš„å€¼å¡«å……ç¼ºå¤±çš„å€¼ã€‚

## å¦‚ä½•å¤„ç†è¿ç»­ç‰¹å¾

æ•°æ®é›†ä¸­çš„è¿ç»­è¦ç´ å…·æœ‰ä¸åŒçš„å€¼èŒƒå›´ã€‚è¿ç»­ç‰¹å¾çš„å¸¸è§ä¾‹å­æœ‰å¹´é¾„ã€å·¥èµ„ã€ä»·æ ¼å’Œèº«é«˜ã€‚

åœ¨è®­ç»ƒæœºå™¨å­¦ä¹ ç®—æ³•ä¹‹å‰ï¼Œå¤„ç†æ•°æ®é›†ä¸­çš„è¿ç»­ç‰¹å¾éå¸¸é‡è¦ã€‚å¦‚æœæ‚¨ä½¿ç”¨ä¸åŒèŒƒå›´çš„å€¼æ¥è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹å°†ä¸ä¼šå¾ˆå¥½åœ°æ‰§è¡Œã€‚

å½“æˆ‘è¯´ä¸åŒèŒƒå›´çš„å€¼æ—¶ï¼Œæˆ‘æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿå‡è®¾ä½ æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå®ƒæœ‰ä¸¤ä¸ªè¿ç»­çš„ç‰¹å¾ï¼Œ**å¹´é¾„**å’Œ**è–ªæ°´**ã€‚å¹´é¾„èŒƒå›´ä¸å·¥èµ„èŒƒå›´ä¸åŒï¼Œè¿™å¯èƒ½ä¼šå¼•èµ·é—®é¢˜ã€‚

![new-op212](img/90a0c2494797defe27d80d5339c86eb5.png)

ä»¥ä¸‹æ˜¯å¤„ç†è¿ç»­ç‰¹å¾çš„ä¸€äº›å¸¸ç”¨æ–¹æ³•:

### æœ€å°-æœ€å¤§å½’ä¸€åŒ–

å¯¹äºè¦ç´ ä¸­çš„æ¯ä¸ªå€¼ï¼Œæœ€å°-æœ€å¤§å½’ä¸€åŒ–ä¼šå‡å»è¦ç´ ä¸­çš„æœ€å°å€¼ï¼Œç„¶åé™¤ä»¥å…¶èŒƒå›´ã€‚è¯¥èŒƒå›´æ˜¯åŸå§‹æœ€å¤§å€¼å’ŒåŸå§‹æœ€å°å€¼ä¹‹é—´çš„å·®å€¼ã€‚

![Picture2](img/ad49a8908a41810b2ea7b29b731a62bc.png)

æœ€åï¼Œå®ƒç¼©æ”¾åœ¨ **0** å’Œ **1 ä¹‹é—´çš„å›ºå®šèŒƒå›´å†…çš„æ‰€æœ‰å€¼ã€‚**

æ‚¨å¯ä»¥ä½¿ç”¨ Scikit-learn ä¸­çš„ **[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)** æ–¹æ³•ï¼Œé€šè¿‡å°†æ¯ä¸ªè¦ç´ ç¼©æ”¾åˆ°ç»™å®šèŒƒå›´æ¥å˜æ¢è¦ç´ :

```
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 4 samples/observations and 2 variables/features
data = np.array([[4, 6], [11, 34], [10, 17], [1, 5]])

# create scaler method
scaler = MinMaxScaler(feature_range=(0,1))

# fit and transform the data
scaled_data = scaler.fit_transform(data)

print(scaled_data)

# [[0.3        0.03448276]
#  [1\.         1\.        ] 
#  [0.9        0.4137931 ] 
#  [0\.         0\.        ]]
```

å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬çš„æ•°æ®å·²ç»è¢«è½¬æ¢ï¼ŒèŒƒå›´åœ¨ **0** å’Œ **1** ä¹‹é—´ã€‚

### æ ‡å‡†åŒ–

æ ‡å‡†åŒ–ç¡®ä¿æ¯ä¸ªç‰¹å¾éƒ½æœ‰ä¸€ä¸ªå¹³å‡å€¼ **0** å’Œä¸€ä¸ªæ ‡å‡†åå·® **1** ï¼Œä½¿æ‰€æœ‰ç‰¹å¾è¾¾åˆ°ç›¸åŒçš„é‡çº§ã€‚

å¦‚æœç‰¹å¾çš„æ ‡å‡†å·®ä¸åŒ*ï¼Œå®ƒä»¬çš„èŒƒå›´ä¹Ÿä¼šä¸åŒã€‚*

*![image-24](img/bc19bf37ebda11e088318c702d664d9a.png)

x = observation, Î¼ = mean , Ïƒ = standard deviation* 

*æ‚¨å¯ä»¥ä½¿ç”¨ Scikit-learn ä¸­çš„ **[æ ‡å‡†ç¼©æ”¾å™¨](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)** æ–¹æ³•ï¼Œé€šè¿‡ç§»é™¤å¹³å‡å€¼å¹¶ç¼©æ”¾è‡³æ ‡å‡†åå·® **1** æ¥æ ‡å‡†åŒ–ç‰¹å¾:*

```
*`from sklearn.preprocessing import StandardScaler
import numpy as np

# 4 samples/observations and 2 variables/features
data = np.array([[4, 1], [11, 1], [10, 4], [1, 11]])

# create scaler method 
scaler = StandardScaler()

# fit and transform the data
scaled_data = scaler.fit_transform(data)

print(scaled_data)

# [[-0.60192927 -0.79558708]
#  [ 1.08347268 -0.79558708] 
#  [ 0.84270097 -0.06119901] 
#  [-1.32424438  1.65237317]]`*
```

*è®©æˆ‘ä»¬éªŒè¯æ¯ä¸ªç‰¹å¾(åˆ—)çš„å¹³å‡å€¼æ˜¯ **0** :*

```
*`print(scaled_data.mean(axis=0))`*
```

*`[0\. 0.]`*

*å¹¶ä¸”æ¯ä¸ªç‰¹å¾(åˆ—)çš„æ ‡å‡†åå·®æ˜¯ **1** :*

```
*`print(scaled_data.std(axis=0))`*
```

*`[1\. 1.]`*

## *å¦‚ä½•å¤„ç†åˆ†ç±»ç‰¹å¾*

*åˆ†ç±»ç‰¹å¾è¡¨ç¤ºå¯ä»¥åˆ†ç»„çš„æ•°æ®ç±»å‹ã€‚æ¯”å¦‚æ€§åˆ«ï¼Œå—æ•™è‚²ç¨‹åº¦ã€‚*

*ä»»ä½•éæ•°å­—å€¼éƒ½éœ€è¦*è½¬æ¢*ä¸ºæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼Œä»¥ä¾¿åœ¨å¤§å¤šæ•°æœºå™¨å­¦ä¹ åº“ä¸­ä½¿ç”¨ã€‚*

*å¤„ç†åˆ†ç±»ç‰¹å¾çš„å¸¸ç”¨æ–¹æ³•æœ‰:*

### *æ ‡ç­¾ç¼–ç *

*æ ‡ç­¾ç¼–ç åªæ˜¯å°†ä¸€åˆ—ä¸­çš„æ¯ä¸ªåˆ†ç±»å€¼è½¬æ¢æˆä¸€ä¸ªæ•°å­—ã€‚*

*å»ºè®®ä½¿ç”¨æ ‡ç­¾ç¼–ç å°†å…¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶å˜é‡ã€‚*

*åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Scikit-learn ä¸­çš„ **[LableEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)** å°†åˆ†ç±»å€¼è½¬æ¢ä¸ºäºŒè¿›åˆ¶:*

```
*`# import packages
import numpy as np 
import pandas as pd 
from sklearn.preprocessing import LabelEncoder

# intialise data of lists.
data = {'Gender':['male', 'female', 'female', 'male','male'],
        'Country':['Tanzania','Kenya', 'Tanzania', 'Tanzania','Kenya']}

# Create DataFrame
data = pd.DataFrame(data)

# create label encoder object
le = LabelEncoder()

data['Gender']= le.fit_transform(data['Gender'])
data['Country']= le.fit_transform(data['Country'])

print(data)`* 
```

*![hhhjkk-1](img/cd9a2b07df9d90768e1edbaacc2efdfe.png)

Transformed Data* 

### *ä¸€æ¬¡çƒ­ç¼–ç *

*åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¡¨ç¤ºåˆ†ç±»å˜é‡æœ€å¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€é”®ç¼–ç ï¼Œæˆ– N é€‰ 1 ç¼–ç æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºå“‘å˜é‡ã€‚*

*è™šæ‹Ÿå˜é‡èƒŒåçš„æ€æƒ³æ˜¯ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå€¼ä¸º 0 å’Œ 1 çš„æ–°ç‰¹å¾æ›¿æ¢åˆ†ç±»å˜é‡ã€‚*

*![image-25](img/77eecc8bd0c3405c8706012511df4048.png)*

*åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Scikit-learn åº“ä¸­çš„ç¼–ç å™¨ã€‚[](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)****[label encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)**å°†å¸®åŠ©æˆ‘ä»¬ä»æ•°æ®ä¸­åˆ›å»ºæ ‡ç­¾çš„æ•´æ•°ç¼–ç ï¼Œè€Œ[](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)****[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)**å°†åˆ›å»ºæ•´æ•°ç¼–ç å€¼çš„ä¸€æ¬¡æ€§ç¼–ç ã€‚*****

```
*****`# import packages 
import numpy as np 
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# define example
data = np.array(['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot'])

# integer encode
label_encoder = LabelEncoder()

#fit and transform the data
integer_encoded = label_encoder.fit_transform(data)
print(integer_encoded)

# one-hot encode
onehot_encoder = OneHotEncoder(sparse=False)

#reshape the data
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)

#fit and transform the data
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

print(onehot_encoded)`*****
```

*****è¿™æ˜¯ **LabelEncoder** æ–¹æ³•å¯¹**æ•´æ•°ç¼–ç **çš„è¾“å‡º:*****

*****`[0 0 2 0 1 1 2 0 2 1]`*****

*****è¿™æ˜¯é€šè¿‡ **OneHotEncoder** æ–¹æ³•å¾—åˆ°çš„**onehotencoded**çš„è¾“å‡º:*****

```
*****`[[1\. 0\. 0.] 
 [1\. 0\. 0.] 
 [0\. 0\. 1.] 
 [1\. 0\. 0.] 
 [0\. 1\. 0.] 
 [0\. 1\. 0.] 
 [0\. 0\. 1.] 
 [1\. 0\. 0.] 
 [0\. 0\. 1.] 
 [0\. 1\. 0.]]`*****
```

# *****ä»€ä¹ˆæ˜¯ç‰¹å¾é€‰æ‹©ï¼Ÿ*****

*****è¦ç´ é€‰æ‹©æ˜¯è‡ªåŠ¨æˆ–æ‰‹åŠ¨é€‰æ‹©å¯¹é¢„æµ‹å˜é‡æˆ–è¾“å‡ºè´¡çŒ®æœ€å¤§çš„è¦ç´ çš„è¿‡ç¨‹ã€‚*****

*****![1_XHHToil9E5EFeEh0H0rnjA](img/1117394d2cc1e91889c9c192f738ec09.png)*****

*****æ•°æ®ä¸­åŒ…å«ä¸ç›¸å…³çš„ç‰¹å¾ä¼š*é™ä½*æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚*****

*****ä½¿ç”¨ç‰¹å¾é€‰æ‹©çš„ä¸»è¦åŸå› æ˜¯:*****

*   *****å®ƒä½¿æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿæ›´å¿«åœ°è®­ç»ƒã€‚*****
*   *****å®ƒé™ä½äº†æ¨¡å‹çš„å¤æ‚æ€§ï¼Œä½¿å…¶æ›´å®¹æ˜“è§£é‡Šã€‚*****
*   *****å¦‚æœé€‰æ‹©äº†æ­£ç¡®çš„å­é›†ï¼Œå°±å¯ä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚*****
*   *****å®ƒå‡å°‘äº†è¿‡åº¦æ‹Ÿåˆã€‚*****

> *****â€œæˆ‘é€šè¿‡é€‰æ‹©æ‰€æœ‰ç‰¹å¾å‡†å¤‡äº†ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘å¾—åˆ°äº†å¤§çº¦ **65%** çš„å‡†ç¡®åº¦ï¼Œè¿™å¯¹äºä¸€ä¸ªé¢„æµ‹æ¨¡å‹æ¥è¯´ä¸æ˜¯å¾ˆå¥½ï¼Œåœ¨åšäº†ä¸€äº›ç‰¹å¾é€‰æ‹©å’Œç‰¹å¾å·¥ç¨‹è€Œæ²¡æœ‰å¯¹æˆ‘çš„æ¨¡å‹ä»£ç åšä»»ä½•é€»è¾‘æ›´æ”¹ä¹‹åï¼Œæˆ‘çš„å‡†ç¡®åº¦è·ƒå‡åˆ°äº† **81%** ï¼Œè¿™éå¸¸ä»¤äººå°è±¡æ·±åˆ»â€â€”â€”ä½œè€… Raheel Shaikh*****

*****ç‰¹å¾é€‰æ‹©çš„å¸¸ç”¨æ–¹æ³•æœ‰:*****

### *****å•å˜é‡é€‰æ‹©*****

*****ç»Ÿè®¡æµ‹è¯•æœ‰åŠ©äºé€‰æ‹©ä¸æ•°æ®é›†ä¸­çš„ç›®æ ‡è¦ç´ å…³ç³»æœ€å¯†åˆ‡çš„ç‹¬ç«‹è¦ç´ ã€‚æ¯”å¦‚å¡æ–¹æ£€éªŒã€‚*****

*****Scikit-learn åº“æä¾›äº† **[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)** ç±»ï¼Œè¯¥ç±»å¯ç”¨äºä¸€å¥—ä¸åŒçš„ç»Ÿè®¡æµ‹è¯•ï¼Œä»¥é€‰æ‹©ç‰¹å®šæ•°é‡çš„ç‰¹æ€§ã€‚*****

*****åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¸¦æœ‰å¡æ°æ£€éªŒçš„ **SelectKBest** ç±»æ¥å¯»æ‰¾è™¹è†œæ•°æ®é›†çš„æœ€ä½³ç‰¹å¾:*****

```
*****`# Load packages
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# Load iris data
iris_dataset = load_iris()

# Create features and target
X = iris_dataset.data
y = iris_dataset.target

# Convert to categorical data by converting data to integers
X = X.astype(int)

# Two features with highest chi-squared statistics are selected
chi2_features = SelectKBest(chi2, k = 2)
X_kbest_features = chi2_features.fit_transform(X, y)

# Reduced features
print('Original feature number:', X.shape[1])
print('Reduced feature number:', X_kbest_features.shape[1])`*****
```

*****åŸå§‹ç‰¹å¾å·:4
å‡å°‘çš„ç‰¹å¾å·:2*****

*****å¦‚æ‚¨æ‰€è§ï¼Œå¡æ–¹æ£€éªŒå¸®åŠ©æˆ‘ä»¬ä»æœ€åˆçš„ 4 ä¸ªç‰¹å¾ä¸­é€‰æ‹©å‡º**ä¸¤ä¸ªä¸ç›®æ ‡ç‰¹å¾å…³ç³»æœ€å¯†åˆ‡çš„é‡è¦ç‹¬ç«‹ç‰¹å¾**ã€‚*****

*****ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºå¡æ–¹æ£€éªŒçš„çŸ¥è¯†:[æœºå™¨å­¦ä¹ å¡æ–¹æ£€éªŒçš„æ¸©å’Œä»‹ç»](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/)ã€‚*****

### *****ç‰¹å¾é‡è¦æ€§*****

*****è¦ç´ é‡è¦æ€§ä¸ºæ•°æ®çš„æ¯ä¸ªè¦ç´ æä¾›äº†ä¸€ä¸ªåˆ†æ•°ã€‚åˆ†æ•°è¶Šé«˜ï¼Œè¯¥åŠŸèƒ½å¯¹ä½ çš„ç›®æ ‡åŠŸèƒ½è¶Šé‡è¦æˆ–**ç›¸å…³**ã€‚*****

*****ç‰¹å¾é‡è¦æ€§æ˜¯ä¸€ä¸ªå†…ç½®ç±»ï¼Œå¸¦æœ‰åŸºäºæ ‘çš„åˆ†ç±»å™¨ï¼Œä¾‹å¦‚:*****

*   *****éšæœºæ£®æ—åˆ†ç±»å™¨*****
*   *****é¢å¤–æ ‘åˆ†ç±»å™¨*****

*****åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠé¢å¤–çš„æ ‘åˆ†ç±»å™¨è®­ç»ƒåˆ° iris æ•°æ®é›†ä¸­ï¼Œå¹¶ä½¿ç”¨å†…ç½®çš„ç±»**ã€‚feature_importances_** è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§:*****

```
*****`# Load libraries
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.ensemble import ExtraTreesClassifier

# Load iris data
iris_dataset = load_iris()

# Create features and target
X = iris_dataset.data
y = iris_dataset.target

# Convert to categorical data by converting data to integers
X = X.astype(int)

 # Building the model
extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,
                                        criterion ='entropy', max_features = 2)

# Training the model
extra_tree_forest.fit(X, y)

# Computing the importance of each feature
feature_importance = extra_tree_forest.feature_importances_

# Normalizing the individual importances
feature_importance_normalized = np.std([tree.feature_importances_ for tree in 
                                        extra_tree_forest.estimators_],
                                        axis = 0)

# Plotting a Bar Graph to compare the models
plt.bar(iris_dataset.feature_names, feature_importance_normalized)
plt.xlabel('Feature Labels')
plt.ylabel('Feature Importances')
plt.title('Comparison of different Feature Importances')
plt.show()`***** 
```

*****![feature-important](img/cd34fadd6463ed9991c563707a32d2f4.png)

Important features***** 

*****ä¸Šå›¾æ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾æ˜¯****èŠ±ç“£é•¿åº¦(cm)****ï¼Œæœ€ä¸é‡è¦çš„ç‰¹å¾æ˜¯ ***ã€è¼ç‰‡å®½åº¦(cms)*** ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨æœ€é‡è¦çš„åŠŸèƒ½æ¥è®­ç»ƒæ‚¨çš„æ¨¡å‹å¹¶è·å¾—æœ€ä½³æ€§èƒ½ã€‚*******

### ******ç›¸å…³çŸ©é˜µçƒ­å›¾******

******ç›¸å…³æ€§æ˜¾ç¤ºäº†ç‰¹å¾ä¹‹é—´æˆ–ä¸ç›®æ ‡ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚******

******ç›¸å…³æ€§å¯ä»¥æ˜¯æ­£çš„(å¢åŠ ä¸€ä¸ªç‰¹å¾å€¼ä¼šå¢åŠ ç›®æ ‡å˜é‡çš„å€¼)ï¼Œä¹Ÿå¯ä»¥æ˜¯è´Ÿçš„(å¢åŠ ä¸€ä¸ªç‰¹å¾å€¼ä¼šå‡å°‘ç›®æ ‡å˜é‡çš„å€¼)ã€‚******

******åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Scikit-learn åº“ä¸­çš„ Boston house prices æ•°æ®é›†å’Œ pandas ä¸­çš„ **[corr()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)** æ–¹æ³•æ¥æŸ¥æ‰¾ dataframe ä¸­æ‰€æœ‰è¦ç´ çš„æˆå¯¹ç›¸å…³æ€§:******

```
****`# Load libraries
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
import seaborn as sns

# load boston data
boston_dataset = load_boston()

# create a daframe for boston data
boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)

# Convert to categorical data by converting data to integers
#X = X.astype(int)

#ploting the heatmap for correlation
ax = sns.heatmap(boston.corr().round(2), annot=True)`**** 
```

******![1_Fbfj8xjr-PwQnfjQ4CBY_g](img/c522726a63ebb8c422cffdc0fcc36148.png)******

******ç›¸å…³ç³»æ•°èŒƒå›´ä»-1 åˆ° 1ã€‚å¦‚æœè¯¥å€¼æ¥è¿‘ 1ï¼Œåˆ™æ„å‘³ç€è¿™ä¸¤ä¸ªç‰¹å¾ä¹‹é—´æœ‰å¾ˆå¼ºçš„æ­£ç›¸å…³æ€§ã€‚å½“æ¥è¿‘-1 æ—¶ï¼Œç‰¹å¾å…·æœ‰å¾ˆå¼ºçš„è´Ÿç›¸å…³æ€§ã€‚

åœ¨ä¸Šå›¾ä¸­ï¼Œå¯ä»¥çœ‹åˆ°**ç¨**å’Œ **RAD** ç‰¹å¾æœ‰ä¸€ä¸ª s *trong æ­£ç›¸å…³*å’Œ **DIS** å’Œ **NOX** ç‰¹å¾æœ‰ä¸€ä¸ª*å¼ºè´Ÿç›¸å…³ã€‚*******

******å¦‚æœæ‚¨å‘ç°æ•°æ®é›†ä¸­æœ‰ä¸€äº›ç›¸äº’å…³è”çš„è¦ç´ ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¼ è¾¾äº†ç›¸åŒçš„ä¿¡æ¯ã€‚å»ºè®®å»æ‰å…¶ä¸­ä¸€ä¸ªã€‚******

******ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºè¿™æ–¹é¢çš„å†…å®¹:[åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œä¸ºä»€ä¹ˆæœ‰å…³è”çš„ç‰¹å¾æ˜¯ä¸å¥½çš„ï¼Ÿ](https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features)******

## ******ç»“è®º******

******æˆ‘åœ¨æœ¬æ–‡ä¸­è§£é‡Šçš„æ–¹æ³•å°†å¸®åŠ©æ‚¨å‡†å¤‡å¤§éƒ¨åˆ†çš„ç»“æ„åŒ–æ•°æ®é›†ã€‚ä½†æ˜¯å¦‚æœæ‚¨æ­£åœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®é›†ï¼Œæ¯”å¦‚å›¾åƒã€æ–‡æœ¬å’ŒéŸ³é¢‘ï¼Œæ‚¨å°†ä¸å¾—ä¸å­¦ä¹ æœ¬æ–‡ä¸­æ²¡æœ‰è§£é‡Šçš„ä¸åŒæ–¹æ³•ã€‚******

******ä»¥ä¸‹æ–‡ç« å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¸ºæœºå™¨å­¦ä¹ é¡¹ç›®å‡†å¤‡å›¾åƒæˆ–æ–‡æœ¬æ•°æ®é›†:******

*   ******[ä¸º CNN å‡†å¤‡å’Œæ‰©å……å›¾åƒæ•°æ®çš„æœ€ä½³å®è·µ-Jason Brownlee](https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/)******
*   ******[å›¾åƒé¢„å¤„ç†-å¡åŠªç›ç‹å­](https://towardsdatascience.com/image-pre-processing-c1aec0be3edf)******
*   ******[è‡ªç„¶è¯­è¨€å¤„ç†æ–‡æœ¬é¢„å¤„ç†:å®ç”¨æŒ‡å—å’Œæ¨¡æ¿â€”â€”ç¿å®¶è±ª](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79)******
*   ******[å¦‚ä½•ä½¿ç”¨ Texthero ä¸ºæ‚¨çš„ NLP é¡¹ç›®å‡†å¤‡åŸºäºæ–‡æœ¬çš„æ•°æ®é›†-Davis David](https://www.freecodecamp.org/news/how-to-work-and-understand-text-based-dataset-with-texthero/)******

**********ç¥è´º**** ğŸ‘ğŸ‘ ****ã€**** ä½ å·²ç»ç†¬åˆ°è¿™ç¯‡æ–‡ç« ç»“æŸäº†ï¼æˆ‘å¸Œæœ›ä½ å­¦åˆ°äº†ä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œå¯¹ä½ çš„ä¸‹ä¸€ä¸ªæœºå™¨å­¦ä¹ æˆ–æ•°æ®ç§‘å­¦é¡¹ç›®æœ‰æ‰€å¸®åŠ©ã€‚******

****å¦‚æœä½ å­¦åˆ°äº†æ–°çš„ä¸œè¥¿æˆ–è€…å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œè¯·åˆ†äº«ç»™å…¶ä»–äººçœ‹ã€‚åœ¨é‚£ä¹‹å‰ï¼Œä¸‹æœŸå¸–å­å†è§ï¼****

****ä½ ä¹Ÿå¯ä»¥åœ¨ Twitter ä¸Šæ‰¾åˆ°æˆ‘ [@Davis_McDavid](https://twitter.com/Davis_McDavid) ã€‚****

****è€Œä¸”ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šç±»ä¼¼è¿™æ ·çš„æ–‡ç« [ã€‚](https://www.freecodecamp.org/news/author/davis/)****